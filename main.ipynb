{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up environment\n",
    "%pip install --upgrade tensorflow\n",
    "%pip install pillow\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, LeakyReLU, BatchNormalization, Lambda, Input\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, size=(128, 64)):  # Ensure size matches the expected input shape\n",
    "    images = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "            img = Image.open(os.path.join(directory, filename))\n",
    "            img = img.convert('L')  # Convert to grayscale\n",
    "            img = img.resize(size, Image.Resampling.LANCZOS)  # Resize the image\n",
    "            img_array = np.array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=-1)  # Add channel dimension\n",
    "            images.append(img_array)\n",
    "    return np.array(images) / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_res_path = r\"C:\\Users\\kimam\\Desktop\\Project\\datasets\\hr_images\"\n",
    "low_res_path = r\"C:\\Users\\kimam\\Desktop\\Project\\datasets\\lr_images\"\n",
    "\n",
    "# Load high-resolution and low-resolution images\n",
    "high_res_images = load_images(high_res_path, size=(512, 256))  # Adjust the size as needed\n",
    "low_res_images = load_images(low_res_path, size=(128, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to check and reshape the images\n",
    "print(\"Original shape:\", low_res_images.shape)\n",
    "\n",
    "if low_res_images.shape[1:3] != (128, 64):\n",
    "    # Reshape the images\n",
    "    low_res_images = np.reshape(low_res_images, newshape=(-1, 128, 64, 1))\n",
    "    print(\"Reshaped to:\", low_res_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator Model (BARCNN)\n",
    "\n",
    "def pixel_shuffle(scale):\n",
    "    return lambda x: tf.nn.depth_to_space(x, scale)\n",
    "\n",
    "def build_generator():\n",
    "    inputs = Input(shape=(128, 64, 1))  # Starting with 128x64x1 grayscale images\n",
    "\n",
    "    # 128x64x1 to 128x64x256\n",
    "    x = Conv2D(256, (3, 3), padding='same')(inputs)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # 128x64x256 to 256x128x64 (upsampling)\n",
    "    x = Lambda(pixel_shuffle(2))(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # 256x128x64 to 256x128x256\n",
    "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # 256x128x256 to 512x256x64 (upsampling)\n",
    "    x = Lambda(pixel_shuffle(2))(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Final layer: Conv2D to get to the desired channel depth\n",
    "    x = Conv2D(1, (3, 3), padding='same')(x)  # For grayscale images\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "generator = build_generator ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    inputs = Input(shape=(512, 256, 1))  # Starting with 512x256x1 grayscale images\n",
    "\n",
    "    # 512x256x1 to 256x128x128\n",
    "    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # 256x128x128 to 128x64x256\n",
    "    x = Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # 128x64x256 to 64x32x512\n",
    "    x = Conv2D(512, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # 64x32x512 to 32x16x1024\n",
    "    x = Conv2D(1024, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Final layer to reduce to a single feature map\n",
    "    x = Conv2D(1, (3, 3), padding='same')(x)\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Generator and Discriminator Models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Print the summary for the Generator\n",
    "print(\"Generator Model Summary:\")\n",
    "generator.summary()\n",
    "\n",
    "# Print the summary for the Discriminator\n",
    "print(\"\\nDiscriminator Model Summary:\")\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Loss Functions and Optimizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Loss function for the discriminator\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "# Loss function for the generator\n",
    "def generator_loss(fake_output):\n",
    "    return tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = Adam(1e-4)\n",
    "discriminator_optimizer = Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "@tf.function\n",
    "def train_step(low_res_images, high_res_images):\n",
    "# Reshape images if necessary\n",
    "    low_res_images = tf.reshape(low_res_images, [-1, 128, 64, 1])\n",
    "    high_res_images = tf.reshape(high_res_images, [-1, 512, 256, 1])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(low_res_images, training=True)\n",
    "\n",
    "        real_output = discriminator(high_res_images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2  # Number of epochs\n",
    "batch_size = 1  # Batch size\n",
    "\n",
    "# Convert images to TensorFlow datasets for easier batching\n",
    "high_res_dataset = tf.data.Dataset.from_tensor_slices(high_res_images).batch(batch_size)\n",
    "low_res_dataset = tf.data.Dataset.from_tensor_slices(low_res_images).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-resolution images from low-resolution images\n",
    "generated_images = generator.predict(low_res_images[:5])  # Example: Generating 5 images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(target, ref):\n",
    "    # Assume target and ref are TensorFlow tensors\n",
    "    return tf.image.psnr(target, ref, max_val=1.0)\n",
    "def ssim(target, ref):\n",
    "    # Assume target and ref are TensorFlow tensors\n",
    "    return tf.image.ssim(target, ref, max_val=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After generating high-resolution images using the generator\n",
    "low_res_images_tensor = tf.convert_to_tensor(low_res_images, dtype=tf.float32)\n",
    "generated_images = generator.predict(low_res_images_tensor)\n",
    "\n",
    "# Convert high-resolution images to a tensor and cast to float32\n",
    "real_images_tensor = tf.convert_to_tensor(high_res_images, dtype=tf.float32)\n",
    "\n",
    "# Check and reshape images here if necessary\n",
    "print(\"Generated images shape:\", generated_images.shape)\n",
    "print(\"High-resolution images shape:\", high_res_images.shape)\n",
    "if generated_images.shape != high_res_images.shape:\n",
    "    # Reshape code here\n",
    "    generated_images = tf.image.resize(generated_images, (high_res_images.shape[1], high_res_images.shape[2]))\n",
    "    generated_images_tensor = tf.convert_to_tensor(generated_images, dtype=tf.float32)\n",
    "\n",
    "# Calculate PSNR and SSIM\n",
    "average_psnr = psnr(generated_images_tensor, real_images_tensor)\n",
    "average_ssim = ssim(generated_images_tensor, real_images_tensor)\n",
    "\n",
    "print(f\"Average PSNR: {average_psnr.numpy()} dB\")\n",
    "print(f\"Average SSIM: {average_ssim.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# Function to display and save images\n",
    "def display_and_save_images(images, save_dir):\n",
    "    for i, img_tensor in enumerate(images):\n",
    "        # Convert the tensor to a NumPy array and squeeze\n",
    "        img = img_tensor.numpy().squeeze()\n",
    "\n",
    "        # Display the image\n",
    "        #%plt.imshow(img, cmap='gray')\n",
    "        #%plt.axis('off')\n",
    "        #%plt.show()\n",
    "\n",
    "        # Save the image\n",
    "        save_path = os.path.join(save_dir, f'generated_image_{i}.png')\n",
    "        PILImage.fromarray((img * 255).astype(np.uint8)).save(save_path)\n",
    "        print(f\"Image {i} saved to {save_path}\")\n",
    "\n",
    "# Directory to save the images\n",
    "save_directory = r\"C:\\Users\\kimam\\Desktop\\Project\\datasets\\generated_images\"\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Display and save the generated images\n",
    "display_and_save_images(generated_images, save_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
